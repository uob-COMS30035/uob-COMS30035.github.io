\documentclass{article}

\usepackage{amsmath}
\usepackage{bm}

\newcommand{\xvec}{\ensuremath{\mathbf{x}}}
\newcommand{\Xvec}{\ensuremath{\mathbf{X}}}
\newcommand{\Zvec}{\ensuremath{\mathbf{Z}}}
\newcommand{\thetavec}{\ensuremath{{\bm \theta}}}
\newcommand{\xvecn}{\ensuremath{\mathbf{x}_{n}}}
\newcommand{\zvec}{\ensuremath{\mathbf{z}}}
\newcommand{\bmu}{\ensuremath{{\bm \mu}}}
\newcommand{\bcov}{\ensuremath{{\bm \Sigma}}}
\newcommand{\gauss}{\ensuremath{{\cal N}}}


\title{Cribsheet for Machine Learning (COMS30035)\\ Weeks~1--5}
\author{James Cussens}

\newcommand{\yntk}{You need to know}

\begin{document}

\maketitle

\section{Machine Learning Principles}
\label{sec:ml}

\yntk{} what the following terms mean:

\begin{itemize}
\item Unsupervised learning
\item Supervised learning
\item Regression
\item Classification
\item Underfitting
\item Overfitting
\item Model selection
\item Training dataset
\item Validation dataset
\item Test dataset
\item Cross-validation
\item No free lunch theorem
\item Model parameters
\item Parametric model
\item Nonparametric model
\item Likelihood function
\item Maximum likelihood estimation (MLE)
\end{itemize}

\section{Linear Regression}

\yntk{} that the linear regression model is:
\begin{equation}
  \label{eq:lr}
  p(y|\mathbf{x},\mathbf{w}) = \mathbf{w}^{\top}\mathbf{x} + \epsilon 
\end{equation}
where $\epsilon$ has a Gaussian distribution with mean 0:
$\epsilon \sim {\cal N}(0,\sigma^{2})$. You need to know what a
\emph{bias} parameter is and how in (\ref{eq:lr}) it was included in
the parameter vector $\mathbf{w}$ by the addition of a `dummy'
variable which always has the value 1.

You need to understand that we can apply linear regression to a
\emph{feature vector} $\phi(\mathbf{x})$ rather than the original data
$\mathbf{x}$:
\begin{equation}
  \label{eq:lrfeats}
  p(y|\phi(\mathbf{x}),\mathbf{w}) = \mathbf{w}^{\top}\phi(\mathbf{x}) + \epsilon\end{equation}

\yntk:
\begin{enumerate}
\item what the least-squares problem for linear regression is
\item that the solution to this problem has a closed-form (but you
  don't need to memorise this closed-form)
\item and that the least-squares solution is also the maximum
  likelihood solution (you do not need to be able to prove this).
\end{enumerate}

\section{Linear Discriminant}

\yntk{} that when there are two classes Linear Discriminant
computes $y = \mathbf{w}^{\top}\mathbf{x}$ for input $x$ and assigns
$x$ to class $C_1$ if $y\geq 0$ and class $C_2$ otherwise. Parameters
are `learnt' by assuming that:  (1) data for each class have a Gaussian
distribution, (2) these 2 Gaussian distributions have the same
covariance matrix. Parameters can then be found by applying MLE.

\section{Logistic Regression}
\label{sec:logreg}

\yntk{} that the \emph{logistic sigmoid function} (sometimes
called just the \emph{logistic function}) is:
\[
  \sigma(a) = \frac{1}{1+\exp(-a)}
\]
\yntk{} that the logistic regression model for two classes
is:
\begin{equation}
  \label{eq:logreg}
  p(C_{1}|\mathbf{x}) = \sigma(\mathbf{w}^{\top}\mathbf{x}) \qquad
  p(C_{2}|\mathbf{x}) = 1 - p(C_{1}|\mathbf{x})
\end{equation}
\yntk{} that the MLE parameters for logistic regression can be found by
gradient descent.

\section{Neural networks}
\label{sec:nns}

\yntk{} what the following terms mean:
\begin{itemize}
\item Weights
\item Activation function
\item Input layer
\item Hidden layer
\item Output layer
\item Cost function / Loss function
\item Forward pass
\item Backward pass
\item Backpropagation
\item Vanishing gradient problem
\item Exploding gradient problem
\item Gradient clipping
\item Non-saturating activation functions
\item Residual layer / network
\item Parameter initialisation
\item Early stopping
\item Weight decay
\item Dropout
\end{itemize}

\yntk{} that a unit $j$ in a neural network (but not in the input
layer) computes a value $z_j$ by first computing $a_j$, a weighted sum
of its inputs (from the previous layer), and then sending $a_j$ to
some nonlinear \emph{activation function} $h$:
\begin{align}
  \label{eq:preact}
  a_{j} & = \sum_{i}w_{ji}z_{i} \\
  \label{eq:act}
  z_{j} & = h(a_{j}) \\
\end{align}

\yntk{} the \emph{backpropagation formula}:
\begin{equation}
  \label{eq:backprop}
  \delta_{j} = h'(a_{j})\sum_{k}w_{kj}\delta_{k}
\end{equation}
and be able to explain what each of the symbols in this formula represents.

\section{Trees}
\label{sec:trees}

\yntk \dots
\begin{itemize}
\item what a classification tree is
\item what a regression tree is
\item how trees partition the input space
\item that trees are a nonparametric method
\item how the standard CART algorithm for learning trees works,
  including the final pruning stage
\end{itemize}

\section{Kernels and SVMs}
\label{sec:kernels}

\yntk{} what the following terms mean
\begin{itemize}
\item kernel function
\item the kernel trick
\item dual parameter
\item Gram matrix
\item the margin
\item support vectors
\item a soft margin
\item slack variables
\end{itemize}

\yntk \dots
\begin{itemize}
\item the role of the regularisation parameter in soft margins
\item how SVMs can be extended to deal with having more than two classes
\end{itemize}

\section{Probabilistic Graphical Models}
\label{sec:pgms}

\yntk{} what the following terms mean
\begin{itemize}
\item Directed acyclic graph
\item Conditional independence
\item Bayesian network
\item the structure of a Bayesian network
\item the parameters of a Bayesian network
\item child (in a Bayesian network)
\item parent (in a Bayesian network)
\item descendant (in a Bayesian network)
\item path (in a Bayesian network)
\item collider (in a Bayesian network)
\item blocked path (in a Bayesian network)
\end{itemize}

\yntk \dots
\begin{itemize}
\item the factorisation of a joint probability distribution defined by
  the structure of a given Bayesian network
\item how to use plate notation to compactly represent a Bayesian network
\item how to translate a machine learning model (described in words)
  to a Bayesian network
\item how to use d-separation to check for conditional independence
  relations in a Bayesian network.
\end{itemize}

\section{Bayesian machine learning}
\label{sec:bayesian}

\yntk{} what the following terms mean
\begin{itemize}
\item Prior distribution
\item Likelihood
\item Posterior distribution
\end{itemize}

\yntk{} that in the Bayesian approach: the parameters, the data and
any unobserved (latent) variables are all represented as random
variables in a joint probability distribution. Unknown quantities
(parameters and latent variables) are unobserved random variables,
know quantities (the data) are observed random variables.

\section{Sampling and MCMC}
\label{sec:mcmc}

\yntk{} what the following terms mean
\begin{itemize}
\item ancestral sampling
\item rejection sampling
\item Markov chain
\item homogeneous Markov chain
\item initial distribution (in a Markov chain)
\item transition distribution (in a Markov chain)
\item Markov chain Monte Carlo
\item target probability distribution
\item Metropolis-Hastings algorithm
\item Metropolis algorithm
\item proposal distribution
\item acceptance probability
\item burn-in
\item convergence (in context of MCMC)
\end{itemize}

\yntk \dots
\begin{itemize}
\item the equations for the acceptance probability for both the
  Metropolis and Metropolis-Hastings algorithms.
\item how a sample from a distribution can be used to approximate an
  expected value defined by that distribution
\item that in MCMC we sample from a \textbf{sequence} of distributions
  and that the samples are not independent
\item why we throw away samples in the burn-in
\item why we typically run several chains when doing MCMC
\item that $\hat{R}$ is a value computed from an MCMC run used to
  check for convergence; if the run has been successful (i.e.\ there's
  been convergence) it will be close to 1.
\end{itemize}

\section{k-means and Gaussian mixtures}
\label{sec:kmeansetc}

\yntk{} what the following terms mean
\begin{itemize}
\item clustering
\item soft clustering
\item Gaussian mixture model
\item mixing coefficient
\item responsibility (in context of a mixture model)
\end{itemize}

\yntk \dots
\begin{itemize}
\item how the k-means algorithm works
\item that one can do soft clustering by applying MLE to a Gaussian
  mixture model
\end{itemize}

\section{The EM algorithm}
\label{sec:em}

\yntk{} that
\begin{itemize}
\item the EM algorithm is an iterative algorithm that attempts to find
  a value of $\thetavec$ that maximises the \emph{log-likelihood}:
  $\ln p(\Xvec | \thetavec)$, where $\Xvec$ is observed data.
\item there is no guarantee that the EM algorithm will succeed in
  maximising the log-likelihood. It may converge to a local maximum
  which is not a global maximum of the log-likelihood function.
\end{itemize}

If you are given any or all of the following three EM-related equations:
  \[
    \ln p(\Xvec | \thetavec)  = {\cal L}(q, \thetavec) + \mathrm{KL}(q
      || p)
  \]
  \begin{eqnarray*}
    {\cal L}(q, \thetavec) & = & \sum_{\Zvec} q(\Zvec) \ln \left\{
      \frac{p(\Xvec,\Zvec| \thetavec)}{q(\Zvec)} \right\} \\
    \mathrm{KL}(q || p) & = & -\sum_{\Zvec} q(\Zvec) \ln \left\{
      \frac{p(\Zvec|\Xvec, \thetavec)}{q(\Zvec)} \right\}
  \end{eqnarray*}
you should be able to explain what each of the symbols in these
equations represent. It can be helpful to you to simply memorise these
three equations.

\yntk{} that
 \begin{itemize}
  \item $\mathrm{KL}(q
      || p) \geq 0$ for any choice of $q$, so ${\cal L}(q, \thetavec)
      \leq \ln p(\Xvec | \thetavec)$.
    \item In the E-step we increase ${\cal L}(q, \thetavec)$ by
      updating $q$ (and leaving $\thetavec$ fixed).
    \item In the M-step we increase ${\cal L}(q, \thetavec)$ by
      updating $\thetavec$ (and leaving $q$ fixed).
    \item After the E-step we have
      ${\cal L}(q, \thetavec) = \ln p(\Xvec | \thetavec)$ (and so $\mathrm{KL}(q
      || p) = 0$), so that in
      the following M-step increasing ${\cal L}(q, \thetavec)$ will
      also increase $\ln p(\Xvec | \thetavec)$.
  \end{itemize}
 

\end{document}
