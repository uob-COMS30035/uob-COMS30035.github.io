\documentclass{article}

\usepackage{amsmath, amssymb}
\usepackage{bm}

\newcommand{\xvec}{\ensuremath{\mathbf{x}}}
\newcommand{\Xvec}{\ensuremath{\mathbf{X}}}
\newcommand{\Zvec}{\ensuremath{\mathbf{Z}}}
\newcommand{\thetavec}{\ensuremath{{\bm \theta}}}
\newcommand{\xvecn}{\ensuremath{\mathbf{x}_{n}}}
\newcommand{\zvec}{\ensuremath{\mathbf{z}}}
\newcommand{\bmu}{\ensuremath{{\bm \mu}}}
\newcommand{\bcov}{\ensuremath{{\bm \Sigma}}}
\newcommand{\gauss}{\ensuremath{{\cal N}}}

\newcommand{\bs}{\boldsymbol}



\title{Cribsheet for Machine Learning (COMS30035)}
\author{James Cussens}

\newcommand{\yntk}{You need to know}

\begin{document}

\maketitle

\section{Machine Learning Principles}
\label{sec:ml}

\yntk{} what the following terms mean:

\begin{itemize}
\item Unsupervised learning
\item Supervised learning
\item Regression
\item Classification
\item Underfitting
\item Overfitting
\item Model selection
\item Training dataset
\item Validation dataset
\item Test dataset
\item Cross-validation
\item No free lunch theorem
\item Model parameters
\item Parametric model
\item Nonparametric model
\item Likelihood function
\item Maximum likelihood estimation (MLE)
\end{itemize}

\section{Linear Regression}

\yntk{} that the linear regression model is:
\begin{equation}
  \label{eq:lr}
  p(y|\mathbf{x},\mathbf{w}) = \mathbf{w}^{\top}\mathbf{x} + \epsilon 
\end{equation}
where $\epsilon$ has a Gaussian distribution with mean 0:
$\epsilon \sim {\cal N}(0,\sigma^{2})$. You need to know what a
\emph{bias} parameter is and how in (\ref{eq:lr}) it was included in
the parameter vector $\mathbf{w}$ by the addition of a `dummy'
variable which always has the value 1.

You need to understand that we can apply linear regression to a
\emph{feature vector} $\phi(\mathbf{x})$ rather than the original data
$\mathbf{x}$:
\begin{equation}
  \label{eq:lrfeats}
  p(y|\phi(\mathbf{x}),\mathbf{w}) = \mathbf{w}^{\top}\phi(\mathbf{x}) + \epsilon\end{equation}

\yntk:
\begin{enumerate}
\item what the least-squares problem for linear regression is
\item that the solution to this problem has a closed-form (but you
  don't need to memorise this closed-form)
\item and that the least-squares solution is also the maximum
  likelihood solution (you do not need to be able to prove this).
\end{enumerate}

\section{Linear Discriminant}

\yntk{} that when there are two classes Linear Discriminant
computes $y = \mathbf{w}^{\top}\mathbf{x}$ for input $x$ and assigns
$x$ to class $C_1$ if $y\geq 0$ and class $C_2$ otherwise. Parameters
are `learnt' by assuming that:  (1) data for each class have a Gaussian
distribution, (2) these 2 Gaussian distributions have the same
covariance matrix. Parameters can then be found by applying MLE.

\section{Logistic Regression}
\label{sec:logreg}

\yntk{} that the \emph{logistic sigmoid function} (sometimes
called just the \emph{logistic function}) is:
\[
  \sigma(a) = \frac{1}{1+\exp(-a)}
\]
\yntk{} that the logistic regression model for two classes
is:
\begin{equation}
  \label{eq:logreg}
  p(C_{1}|\mathbf{x}) = \sigma(\mathbf{w}^{\top}\mathbf{x}) \qquad
  p(C_{2}|\mathbf{x}) = 1 - p(C_{1}|\mathbf{x})
\end{equation}
\yntk{} that the MLE parameters for logistic regression can be found by
gradient descent.

\section{Neural networks}
\label{sec:nns}

\yntk{} what the following terms mean:
\begin{itemize}
\item Weights
\item Activation function
\item Input layer
\item Hidden layer
\item Output layer
\item Cost function / Loss function
\item Forward pass
\item Backward pass
\item Backpropagation
\item Vanishing gradient problem
\item Exploding gradient problem
\item Gradient clipping
\item Non-saturating activation functions
\item Residual layer / network
\item Parameter initialisation
\item Early stopping
\item Weight decay
\item Dropout
\end{itemize}

\yntk{} that a unit $j$ in a neural network (but not in the input
layer) computes a value $z_j$ by first computing $a_j$, a weighted sum
of its inputs (from the previous layer), and then sending $a_j$ to
some nonlinear \emph{activation function} $h$:
\begin{align}
  \label{eq:preact}
  a_{j} & = \sum_{i}w_{ji}z_{i} \\
  \label{eq:act}
  z_{j} & = h(a_{j}) \\
\end{align}

\yntk{} the \emph{backpropagation formula}:
\begin{equation}
  \label{eq:backprop}
  \delta_{j} = h'(a_{j})\sum_{k}w_{kj}\delta_{k}
\end{equation}
and be able to explain what each of the symbols in this formula represents.

\section{Trees}
\label{sec:trees}

\yntk \dots
\begin{itemize}
\item what a classification tree is
\item what a regression tree is
\item how trees partition the input space
\item that trees are a nonparametric method
\item how the standard greedy algorithm (CART) for learning trees works,
  including the final pruning stage
\end{itemize}

\section{Kernels and SVMs}
\label{sec:kernels}

\yntk{} what the following terms mean
\begin{itemize}
\item kernel function
\item the kernel trick
\item dual parameter
\item Gram matrix
\item the margin
\item support vectors
\item a soft margin
\item slack variables
\end{itemize}

\yntk \dots
\begin{itemize}
\item the role of the regularisation parameter in soft margins
\item how SVMs can be extended to deal with having more than two classes
\end{itemize}

\section{Probabilistic Graphical Models}
\label{sec:pgms}

\yntk{} what the following terms mean
\begin{itemize}
\item Directed acyclic graph
\item Conditional independence
\item Bayesian network
\item the structure of a Bayesian network
\item the parameters of a Bayesian network
\item child (in a Bayesian network)
\item parent (in a Bayesian network)
\item descendant (in a Bayesian network)
\item path (in a Bayesian network)
\item collider (in a Bayesian network)
\item blocked path (in a Bayesian network)
\end{itemize}

\yntk \dots
\begin{itemize}
\item the factorisation of a joint probability distribution defined by
  the structure of a given Bayesian network
\item how to use plate notation to compactly represent a Bayesian network
\item how to translate a machine learning model (described in words)
  to a Bayesian network
\item how to use d-separation to check for conditional independence
  relations in a Bayesian network.
\end{itemize}

\section{Bayesian machine learning}
\label{sec:bayesian}

\yntk{} what the following terms mean
\begin{itemize}
\item Prior distribution
\item Likelihood
\item Posterior distribution
\end{itemize}

\yntk{} that in the Bayesian approach: the parameters, the data and
any unobserved (latent) variables are all represented as random
variables in a joint probability distribution. Unknown quantities
(parameters and latent variables) are unobserved random variables,
know quantities (the data) are observed random variables.

\section{Sampling and MCMC}
\label{sec:mcmc}

\yntk{} what the following terms mean
\begin{itemize}
\item ancestral sampling
\item rejection sampling
\item Markov chain
\item homogeneous Markov chain
\item initial distribution (in a Markov chain)
\item transition distribution (in a Markov chain)
\item Markov chain Monte Carlo
\item target probability distribution
\item Metropolis-Hastings algorithm
\item Metropolis algorithm
\item proposal distribution
\item acceptance probability
\item burn-in
\item convergence (in context of MCMC)
\end{itemize}

\yntk \dots
\begin{itemize}
\item the equations for the acceptance probability for both the
  Metropolis and Metropolis-Hastings algorithms.
\item how a sample from a distribution can be used to approximate an
  expected value defined by that distribution
\item that in MCMC we sample from a \textbf{sequence} of distributions
  and that the samples are not independent
\item why we throw away samples in the burn-in
\item why we typically run several chains when doing MCMC
\item that $\hat{R}$ is a value computed from an MCMC run used to
  check for convergence; if the run has been successful (i.e.\ there's
  been convergence) it will be close to 1.
\end{itemize}

\section{k-means and Gaussian mixtures}
\label{sec:kmeansetc}

\yntk{} what the following terms mean
\begin{itemize}
\item clustering
\item soft clustering
\item Gaussian mixture model
\item mixing coefficient
\item responsibility (in context of a mixture model)
\end{itemize}

\yntk \dots
\begin{itemize}
\item how the k-means algorithm works
\item that one can do soft clustering by applying MLE to a Gaussian
  mixture model
\end{itemize}

\section{The EM algorithm}
\label{sec:em}

\yntk{} that
\begin{itemize}
\item the EM algorithm is an iterative algorithm that attempts to find
  a value of $\thetavec$ that maximises the \emph{log-likelihood}:
  $\ln p(\Xvec | \thetavec)$, where $\Xvec$ is observed data.
\item there is no guarantee that the EM algorithm will succeed in
  maximising the log-likelihood. It may converge to a local maximum
  which is not a global maximum of the log-likelihood function.
\end{itemize}

If you are given any or all of the following three EM-related equations:
  \[
    \ln p(\Xvec | \thetavec)  = {\cal L}(q, \thetavec) + \mathrm{KL}(q
      || p)
  \]
  \begin{eqnarray*}
    {\cal L}(q, \thetavec) & = & \sum_{\Zvec} q(\Zvec) \ln \left\{
      \frac{p(\Xvec,\Zvec| \thetavec)}{q(\Zvec)} \right\} \\
    \mathrm{KL}(q || p) & = & -\sum_{\Zvec} q(\Zvec) \ln \left\{
      \frac{p(\Zvec|\Xvec, \thetavec)}{q(\Zvec)} \right\}
  \end{eqnarray*}
you should be able to explain what each of the symbols in these
equations represent. It can be helpful to you to simply memorise these
three equations.

\yntk{} that
 \begin{itemize}
  \item $\mathrm{KL}(q
      || p) \geq 0$ for any choice of $q$, so ${\cal L}(q, \thetavec)
      \leq \ln p(\Xvec | \thetavec)$.
    \item In the E-step we increase ${\cal L}(q, \thetavec)$ by
      updating $q$ (and leaving $\thetavec$ fixed).
    \item In the M-step we increase ${\cal L}(q, \thetavec)$ by
      updating $\thetavec$ (and leaving $q$ fixed).
    \item After the E-step we have
      ${\cal L}(q, \thetavec) = \ln p(\Xvec | \thetavec)$ (and so $\mathrm{KL}(q
      || p) = 0$), so that in
      the following M-step increasing ${\cal L}(q, \thetavec)$ will
      also increase $\ln p(\Xvec | \thetavec)$.
  \end{itemize}
 
\section{Hidden Markov models}
\label{sec:hmms}

\yntk{} that
\begin{itemize}
\item A hidden Markov model (HMM) is a state space model where the states
  are discrete.
\item An HMM is defined by an initial distribution over states $p(\bs
  z_1 | \bs \pi)$, emission distributions $p(\bs x_n | \bs z_n,
  \bs\phi)$ and state transition distributions $p(\bs z_n | \bs
  z_{n-1}, \bs A)$ where the joint distribution over states $\bs Z$
  and observations $\bs X$ is:
\[  p(\bs X, \bs Z | \bs A, \bs \pi, \bs \phi) = p(\bs z_1 | \bs \pi) \prod_{n=2}^N p(\bs z_n | \bs z_{n-1}, \bs A) \prod_{n=1}^N p(\bs x_n | \bs z_n, \bs\phi)
\]
where $\bs A$, $\bs\pi$ and $\bs\phi$ are the parameters defining the
initial, emission and transition distributions, respectively.
\item The observations for an HMM can be discrete or continuous and
  there is no restriction on the emission distributions.
\item If the sequence of states $\bs Z$ is not observed then the EM
  algorithm can be used to (attempt to) find values of the parameters
  $(\bs \pi, \bs \phi, \bs A)$ which maximise the log-likelihood $\ln
  p(\bs X | \bs A, \bs\pi, \bs\phi)$.
\item When using the EM algorithm for HMMs it is not necessary (in the
  E step) to compute and fully represent the distribution
  $p(\bs Z|\bs X, \bs\theta^{old})$. Only certain expectations with
  respect to this distribution are required (for the following M
  step). These are computed using the forward-backward algorithm.
\item The forward pass of the forward-backward algorithm computes for each time-step $n>1$ and state value $k$: 
$\alpha(z_{nk})  = p(\bs x_1,...,\bs x_n, z_{n}=k | \bs\pi, \bs A, \bs\phi) \nonumber\\
=  p(\bs x_n | z_{n}=k, \bs\phi_k) \sum_{l=1}^K A_{lk}
\alpha(z_{n-1,l})$
\item The backward pass of the forward-backward algorithm computes for each time-step $n<N$ and state value $k$: $\beta(z_{nk})  = p(\bs x_{n+1},...,\bs x_N |  z_{n}=k, \bs A, \bs\phi )  
 = \sum_{l=1}^K A_{kl} p(\bs x_{n+1} | z_{n+1}=l, \bs\phi_l)  \beta(z_{n+1,l})
$
\item The forward and backward probabilities ($\alpha(z_{nk})$ and
  $\beta(z_{nk})$) can be used to compute the expectations for the E
  step (but you don't need to memorise the relevant equations, nor the
  equations for the M step).
\item There is an algorithm called the Viterbi algorithm which can
  efficiently find, for a particular HMM, the most probable sequence
  of states given a sequence of observations. You do not need to be
  able to describe this algorithm.
\end{itemize}

\section{Linear Dynamical Systems}
\label{sec:lds}

\yntk{} that
\begin{itemize}
\item A linear dynamical system (LDS) is a state space model where the states
  are continuous.
\item The initial state, state transition and emission distributions
  for an LDS are, respectively:
  \begin{enumerate}
  \item $p(\bs z_1) = \mathcal{N}(\bs z_1 | \bs\mu_0, \bs V_0)$;
  \item $p(\bs z_n | \bs z_{n-1}) = \mathcal{N}(\bs z_n | \bs A \bs z_{n-1}, \bs\Gamma)$;
  \item $p(\bs x_n | \bs z_n) = \mathcal{N}(\bs x_n | \bs C \bs z_n, \bs\Sigma)$.
  \end{enumerate}
  where $(\bs\mu_0, \bs V_0, \bs A, \bs \Gamma, \bs C, \bs \Sigma)$ are
  the parameters of the LDS.
\item The LDS analogue to computing HMM forward probabilities is
  called the Kalman filter. (You don't need to memorise the relevant equation.)
\item The LDS analogue to computing HMM backward probabilities is
  called the Kalman smoother. (You don't need to memorise the relevant
  equation.)
\item There is no need for an analogue to the Viterbi algorithm for
  LDS, since the most likely sequence of states is just the sequence
  of most probable states.
\end{itemize}

\section{Ensemble methods}
\label{sec:ensembles}

\yntk{} what the following terms mean:
\begin{itemize}
\item Model selection
\item Bayesian model averaging
\item Bagging
\item Boosting
\item Stacking
\item Hyperparameters
\item Random search for good hyperparameters
\item Grid search for good hyperparameters
\item Random forest
\item Conditional mixture model
\item Mixture of experts
\end{itemize}

You do not need to know about
\begin{itemize}
\item  Bayesian Optimisation
\end{itemize}

\yntk{}
\begin{itemize}
\item How predictions are made using Bayesian model averaging, namely:
  \[
    p(\bs z | \bs X) = \sum_{h=1}^H p(\bs z | \bs X, h)p(h|\bs X)
  \]
\item If $E_{COM}$ is the expected error of a simple ensemble which
  makes predictions as follows: $y_{COM}(\bs x) = \frac{1}{M} \sum_{m=1}^M
  y_m(\bs x)$ and $E_{AV}$ is the average expected error of an
  individual model then $E_{COM} = \frac{1}{M} E_{AV}$ as long as:
  \begin{enumerate}
  \item The errors of each model have zero mean.
  \item The errors of different models are not correlated.
  \end{enumerate}
\item Even if the above 2 assumptions do not hold we still have
  $E_{COM} \leq  E_{AV}$.
\item How bagging works, as described, for example on slide L15:23/46.
\item How AdaBoost works, as described, for example on slide
  L15:30/46.
\item How predictions are made from an AdaBoost ensemble, namely:
  \[
    y_M(\bs x_n) = \sum_{m=1}^M \alpha_m y_m(\bs x_n)
  \]
  where $\alpha_{m} = \ln \left(
    \frac{1-\epsilon_{m}}{\epsilon_{m}}\right)$ and where
  $\epsilon_{m}$ is the weighted error rate of model $m$. 
\item A random forest is a collection of trees built where each
  individual tree is built as follows: a dataset of size $N$ is sampled with
  replacement from the original dataset which has $N$ datapoints; when
  determining a split only a randomly selected set of $d$ features are
  considered where $d \ll D$ where $D$ is the total number of features
  in the data.
\item Predictions from random forests are typically made using the
  mean (for regression) or majority vote (for classification).
\item In a Mixture of Experts model where the component models provide
  a distribution over the target variable, the distribution given by
  the mixture is:
  \[
    p(t_n | \bs x_n, \bs \phi, \bs\pi) = \sum_{k=1}^K \pi_k(\bs x_n) p(t_n | \bs x_n, \bs\phi_k)
  \]
  Note that the weights $\pi_k(\bs x_n)$ depend on the
  input $x_n$.
\item EM can be used to learn the parameters of a Mixture of Experts model.
\end{itemize}


\end{document}
